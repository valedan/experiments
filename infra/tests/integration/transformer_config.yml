epochs: 1
val_interval: 1000
logger:
  flush_interval: 100
  aggregation_interval: 10
optimizer:
  name: "adam"
  learning_rate: 0.001
model:
  architecture: 'transformer'
  d_model: 128
  nhead: 2
  num_decoder_layers: 2
  dim_feedforward: 512
  vocab_size: 100
  context_size: 4000
tokenizer:
  vocab_size: 100
loader:
  dataset: 'tinystories'
  train_frac: 0.01
  val_frac: 0.01
  train_batch: 32
  val_batch: 32
